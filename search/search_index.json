{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Document-to-Podcast Blueprint","text":"<p>Blueprints empower developers to easily integrate AI capabilities into their projects using open-source models and tools.</p> <p>These docs are your companion to mastering the Document-to-Podcast Blueprint\u2014a local-first approach for transforming documents into engaging podcasts.</p>"},{"location":"#built-with","title":"Built with","text":"<ul> <li>Python 3.10+</li> <li>Llama-cpp (text-to-text, i.e script generation)</li> <li>Parler_tts (text-to-speech, i.e audio generation)</li> <li>Streamlit (UI demo)</li> </ul>"},{"location":"#get-started-quickly","title":"\ud83d\ude80 Get Started Quickly","text":""},{"location":"#start-building-your-own-document-to-podcast-pipeline-in-minutes","title":"Start building your own Document-to-Podcast pipeline in minutes:","text":"<ul> <li>Getting Started: Quick setup and installation instructions.</li> </ul>"},{"location":"#understand-the-system","title":"\ud83d\udd0d Understand the System","text":""},{"location":"#dive-deeper-into-how-the-blueprint-works","title":"Dive deeper into how the Blueprint works:","text":"<ul> <li>Step-by-Step Guide: A detailed breakdown of the system\u2019s design and workflow.</li> <li>API Reference: Explore the technical details of the core modules.</li> </ul>"},{"location":"#make-it-yours","title":"\ud83c\udfa8 Make It Yours","text":""},{"location":"#customize-the-blueprint-to-fit-your-needs","title":"Customize the Blueprint to fit your needs:","text":"<ul> <li>Customization Guide: Tailor prompts, voices, and settings to create unique podcasts.</li> </ul>"},{"location":"#join-the-community","title":"\ud83c\udf1f Join the Community","text":""},{"location":"#help-shape-the-future-of-blueprints","title":"Help shape the future of Blueprints:","text":"<ul> <li>Future Features &amp; Contributions: Learn about exciting upcoming features and how to contribute to the project.</li> </ul> <p>Have more questions? Reach out to us on Discord and we'll see how we can help:</p>"},{"location":"#why-blueprints","title":"Why Blueprints?","text":"<p>Blueprints are more than starter code\u2014they\u2019re your gateway to building AI-powered solutions with confidence. With step-by-step guidance, modular design, and open-source tools, we make AI accessible for developers of all skill levels.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#document_to_podcast.preprocessing.data_cleaners","title":"<code>document_to_podcast.preprocessing.data_cleaners</code>","text":""},{"location":"api/#document_to_podcast.preprocessing.data_cleaners.clean_html","title":"<code>clean_html(text)</code>","text":"<p>Clean HTML text.</p> This function removes <ul> <li>scripts</li> <li>styles</li> <li>links</li> <li>meta tags</li> </ul> <p>In addition, it calls clean_with_regex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clean_html(\"&lt;html&gt;&lt;body&gt;&lt;p&gt;Hello,  world!  &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\")\n\"Hello, world!\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The HTML text to clean.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text.</p> Source code in <code>src/document_to_podcast/preprocessing/data_cleaners.py</code> <pre><code>def clean_html(text: str) -&gt; str:\n    \"\"\"Clean HTML text.\n\n    This function removes:\n        - scripts\n        - styles\n        - links\n        - meta tags\n\n    In addition, it calls [clean_with_regex][document_to_podcast.preprocessing.data_cleaners.clean_with_regex].\n\n    Examples:\n        &gt;&gt;&gt; clean_html(\"&lt;html&gt;&lt;body&gt;&lt;p&gt;Hello,  world!  &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\")\n        \"Hello, world!\"\n\n    Args:\n        text (str): The HTML text to clean.\n\n    Returns:\n        str: The cleaned text.\n    \"\"\"\n    soup = BeautifulSoup(text, \"html.parser\")\n    for tag in soup([\"script\", \"style\", \"link\", \"meta\"]):\n        tag.decompose()\n    text = soup.get_text()\n    return clean_with_regex(text)\n</code></pre>"},{"location":"api/#document_to_podcast.preprocessing.data_cleaners.clean_markdown","title":"<code>clean_markdown(text)</code>","text":"<p>Clean Markdown text.</p> This function removes <ul> <li>markdown images</li> </ul> <p>In addition, it calls clean_with_regex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clean_markdown('# Title   with image ![alt text](image.jpg \"Image Title\")')\n\"Title with image\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The Markdown text to clean.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text.</p> Source code in <code>src/document_to_podcast/preprocessing/data_cleaners.py</code> <pre><code>def clean_markdown(text: str) -&gt; str:\n    \"\"\"Clean Markdown text.\n\n    This function removes:\n        - markdown images\n\n    In addition, it calls [clean_with_regex][document_to_podcast.preprocessing.data_cleaners.clean_with_regex].\n\n    Examples:\n        &gt;&gt;&gt; clean_markdown('# Title   with image ![alt text](image.jpg \"Image Title\")')\n        \"Title with image\"\n\n    Args:\n        text (str): The Markdown text to clean.\n\n    Returns:\n        str: The cleaned text.\n    \"\"\"\n    text = re.sub(r'!\\[.*?\\]\\(.*?(\".*?\")?\\)', \"\", text)\n\n    return clean_with_regex(text)\n</code></pre>"},{"location":"api/#document_to_podcast.preprocessing.data_cleaners.clean_with_regex","title":"<code>clean_with_regex(text)</code>","text":"<p>Clean text using regular expressions.</p> This function removes <ul> <li>URLs</li> <li>emails</li> <li>special characters</li> <li>extra spaces</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clean_with_regex(\"\u00a0Hello,   world! http://example.com\")\n\"Hello, world!\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to clean.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text.</p> Source code in <code>src/document_to_podcast/preprocessing/data_cleaners.py</code> <pre><code>def clean_with_regex(text: str) -&gt; str:\n    \"\"\"\n    Clean text using regular expressions.\n\n    This function removes:\n        - URLs\n        - emails\n        - special characters\n        - extra spaces\n\n    Examples:\n        &gt;&gt;&gt; clean_with_regex(\"\\xa0Hello,   world! http://example.com\")\n        \"Hello, world!\"\n\n    Args:\n        text (str): The text to clean.\n\n    Returns:\n        str: The cleaned text.\n    \"\"\"\n    text = re.sub(\n        r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n        \"\",\n        text,\n    )\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\\.[\\w]+\", \"\", text)\n    text = re.sub(r'[^a-zA-Z0-9\\s.,!?;:\"\\']', \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n</code></pre>"},{"location":"api/#document_to_podcast.inference.model_loaders","title":"<code>document_to_podcast.inference.model_loaders</code>","text":""},{"location":"api/#document_to_podcast.inference.model_loaders.load_llama_cpp_model","title":"<code>load_llama_cpp_model(model_id)</code>","text":"<p>Loads the given model_id using Llama.from_pretrained.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = load_llama_cpp_model(\n    \"allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model id to load. Format is expected to be <code>{org}/{repo}/{filename}</code>.</p> required <p>Returns:</p> Name Type Description <code>Llama</code> <code>Llama</code> <p>The loaded model.</p> Source code in <code>src/document_to_podcast/inference/model_loaders.py</code> <pre><code>def load_llama_cpp_model(\n    model_id: str,\n) -&gt; Llama:\n    \"\"\"\n    Loads the given model_id using Llama.from_pretrained.\n\n    Examples:\n        &gt;&gt;&gt; model = load_llama_cpp_model(\n            \"allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf\")\n\n    Args:\n        model_id (str): The model id to load.\n            Format is expected to be `{org}/{repo}/{filename}`.\n\n    Returns:\n        Llama: The loaded model.\n    \"\"\"\n    org, repo, filename = model_id.split(\"/\")\n    model = Llama.from_pretrained(\n        repo_id=f\"{org}/{repo}\",\n        filename=filename,\n        # 0 means that the model limit will be used, instead of the default (512) or other hardcoded value\n        n_ctx=0,\n    )\n    return model\n</code></pre>"},{"location":"api/#document_to_podcast.inference.model_loaders.load_parler_tts_model_and_tokenizer","title":"<code>load_parler_tts_model_and_tokenizer(model_id, device='cpu')</code>","text":"<p>Loads the given model_id using parler_tts.from_pretrained.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model, tokenizer = load_parler_tts_model_and_tokenizer(\"parler-tts/parler-tts-mini-v1\", \"cpu\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model id to load. Format is expected to be <code>{repo}/{filename}</code>.</p> required <code>device</code> <code>str</code> <p>The device to load the model on, such as \"cuda:0\" or \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>PreTrainedModel</code> <code>Tuple[PreTrainedModel, PreTrainedTokenizerBase]</code> <p>The loaded model.</p> Source code in <code>src/document_to_podcast/inference/model_loaders.py</code> <pre><code>def load_parler_tts_model_and_tokenizer(\n    model_id: str, device: str = \"cpu\"\n) -&gt; Tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n    \"\"\"\n    Loads the given model_id using parler_tts.from_pretrained.\n\n    Examples:\n        &gt;&gt;&gt; model, tokenizer = load_parler_tts_model_and_tokenizer(\"parler-tts/parler-tts-mini-v1\", \"cpu\")\n\n    Args:\n        model_id (str): The model id to load.\n            Format is expected to be `{repo}/{filename}`.\n        device (str): The device to load the model on, such as \"cuda:0\" or \"cpu\".\n\n    Returns:\n        PreTrainedModel: The loaded model.\n    \"\"\"\n    model = ParlerTTSForConditionalGeneration.from_pretrained(model_id).to(device)\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    return model, tokenizer\n</code></pre>"},{"location":"api/#document_to_podcast.inference.text_to_text","title":"<code>document_to_podcast.inference.text_to_text</code>","text":""},{"location":"api/#document_to_podcast.inference.text_to_text.text_to_text","title":"<code>text_to_text(input_text, model, system_prompt, return_json=True, stop=None)</code>","text":"<p>Transforms input_text using the given model and system prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to be transformed.</p> required <code>model</code> <code>Llama</code> <p>The model to use for conversion.</p> required <code>system_prompt</code> <code>str</code> <p>The system prompt to use for conversion.</p> required <code>return_json</code> <code>bool</code> <p>Whether to return the response as JSON. Defaults to True.</p> <code>True</code> <code>stop</code> <code>str | list[str] | None</code> <p>The stop token(s).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full transformed text.</p> Source code in <code>src/document_to_podcast/inference/text_to_text.py</code> <pre><code>def text_to_text(\n    input_text: str,\n    model: Llama,\n    system_prompt: str,\n    return_json: bool = True,\n    stop: str | list[str] | None = None,\n) -&gt; str:\n    \"\"\"\n    Transforms input_text using the given model and system prompt.\n\n    Args:\n        input_text (str): The text to be transformed.\n        model (Llama): The model to use for conversion.\n        system_prompt (str): The system prompt to use for conversion.\n        return_json (bool, optional): Whether to return the response as JSON.\n            Defaults to True.\n        stop (str | list[str] | None, optional): The stop token(s).\n\n    Returns:\n        str: The full transformed text.\n    \"\"\"\n    response = chat_completion(\n        input_text, model, system_prompt, return_json, stop=stop, stream=False\n    )\n    return response[\"choices\"][0][\"message\"][\"content\"]\n</code></pre>"},{"location":"api/#document_to_podcast.inference.text_to_text.text_to_text_stream","title":"<code>text_to_text_stream(input_text, model, system_prompt, return_json=True, stop=None)</code>","text":"<p>Transforms input_text using the given model and system prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to be transformed.</p> required <code>model</code> <code>Llama</code> <p>The model to use for conversion.</p> required <code>system_prompt</code> <code>str</code> <p>The system prompt to use for conversion.</p> required <code>return_json</code> <code>bool</code> <p>Whether to return the response as JSON. Defaults to True.</p> <code>True</code> <code>stop</code> <code>str | list[str] | None</code> <p>The stop token(s).</p> <code>None</code> <p>Yields:</p> Name Type Description <code>str</code> <code>str</code> <p>Chunks of the transformed text as they are available.</p> Source code in <code>src/document_to_podcast/inference/text_to_text.py</code> <pre><code>def text_to_text_stream(\n    input_text: str,\n    model: Llama,\n    system_prompt: str,\n    return_json: bool = True,\n    stop: str | list[str] | None = None,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Transforms input_text using the given model and system prompt.\n\n    Args:\n        input_text (str): The text to be transformed.\n        model (Llama): The model to use for conversion.\n        system_prompt (str): The system prompt to use for conversion.\n        return_json (bool, optional): Whether to return the response as JSON.\n            Defaults to True.\n        stop (str | list[str] | None, optional): The stop token(s).\n\n    Yields:\n        str: Chunks of the transformed text as they are available.\n    \"\"\"\n    response = chat_completion(\n        input_text, model, system_prompt, return_json, stop=stop, stream=True\n    )\n    for item in response:\n        if item[\"choices\"][0].get(\"delta\", {}).get(\"content\", None):\n            yield item[\"choices\"][0].get(\"delta\", {}).get(\"content\", None)\n</code></pre>"},{"location":"api/#document_to_podcast.inference.text_to_speech","title":"<code>document_to_podcast.inference.text_to_speech</code>","text":""},{"location":"api/#document_to_podcast.inference.text_to_speech.text_to_speech","title":"<code>text_to_speech(input_text, model, tokenizer, speaker_profile)</code>","text":"<p>Generates a speech waveform using the input_text, a model and a speaker profile to define a distinct voice pattern.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; waveform = text_to_speech(input_text=\"Welcome to our amazing podcast\", model=model, tokenizer=tokenizer, speaker_profile=\"Laura's voice is exciting and fast in delivery with very clear audio and no background noise.\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to convert to speech.</p> required <code>model</code> <code>PreTrainedModel</code> <p>The model used for generating the waveform.</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>The tokenizer used for tokenizing the text in order to send to the model.</p> required <code>speaker_profile</code> <code>str</code> <p>A description used by the ParlerTTS model to configure the speaker profile.</p> required <p>Returns:     numpy array: The waveform of the speech as a 2D numpy array</p> Source code in <code>src/document_to_podcast/inference/text_to_speech.py</code> <pre><code>def text_to_speech(\n    input_text: str,\n    model: PreTrainedModel,\n    tokenizer: PreTrainedTokenizerBase,\n    speaker_profile: str,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generates a speech waveform using the input_text, a model and a speaker profile to define a distinct voice pattern.\n\n    Examples:\n        &gt;&gt;&gt; waveform = text_to_speech(input_text=\"Welcome to our amazing podcast\", model=model, tokenizer=tokenizer, speaker_profile=\"Laura's voice is exciting and fast in delivery with very clear audio and no background noise.\")\n\n    Args:\n        input_text (str): The text to convert to speech.\n        model (PreTrainedModel): The model used for generating the waveform.\n        tokenizer (PreTrainedTokenizerBase): The tokenizer used for tokenizing the text in order to send to the model.\n        speaker_profile (str): A description used by the ParlerTTS model to configure the speaker profile.\n    Returns:\n        numpy array: The waveform of the speech as a 2D numpy array\n    \"\"\"\n    model_id = model.config.name_or_path\n    if \"parler\" in model_id:\n        return _speech_generation_parler(input_text, model, tokenizer, speaker_profile)\n    else:\n        raise NotImplementedError(f\"Model {model_id} not yet implemented for TTS\")\n</code></pre>"},{"location":"api/#document_to_podcast.podcast_maker.script_to_audio","title":"<code>document_to_podcast.podcast_maker.script_to_audio</code>","text":""},{"location":"api/#document_to_podcast.podcast_maker.script_to_audio.parse_script_to_waveform","title":"<code>parse_script_to_waveform(script, podcast_config)</code>","text":"<p>Given a script with speaker identifiers (such as \"Speaker 1\") parse it so that each speaker has its own unique voice and concatenate all the voices in a sequence to form the complete podcast. Args:     script:     podcast_config:</p> <p>Returns: A 2D numpy array containing the whole podcast in waveform format.</p> Source code in <code>src/document_to_podcast/podcast_maker/script_to_audio.py</code> <pre><code>def parse_script_to_waveform(script: str, podcast_config: PodcastConfig):\n    \"\"\"\n    Given a script with speaker identifiers (such as \"Speaker 1\") parse it so that each speaker has its own unique\n    voice and concatenate all the voices in a sequence to form the complete podcast.\n    Args:\n        script:\n        podcast_config:\n\n    Returns: A 2D numpy array containing the whole podcast in waveform format.\n\n    \"\"\"\n    parts = script.split(\"Speaker \")\n    podcast_waveform = []\n    for part in parts:\n        if \":\" in part:\n            speaker_id, speaker_text = part.replace('\"', \"\").split(\":\")\n            speaker_model = podcast_config.speakers[speaker_id].model\n            speaker_tokenizer = podcast_config.speakers[speaker_id].tokenizer\n            speaker_description = podcast_config.speakers[\n                speaker_id\n            ].speaker_description\n            speaker_waveform = text_to_speech(\n                speaker_text, speaker_model, speaker_tokenizer, speaker_description\n            )\n            podcast_waveform.append(speaker_waveform)\n\n    return np.concatenate(podcast_waveform)\n</code></pre>"},{"location":"api/#document_to_podcast.podcast_maker.script_to_audio.save_waveform_as_file","title":"<code>save_waveform_as_file(waveform, sampling_rate, filename)</code>","text":"<p>Save the output of the TTS (a numpy waveform) to a .wav file using the soundfile library.</p> <p>Parameters:</p> Name Type Description Default <code>waveform</code> <code>ndarray</code> <p>2D numpy array of a waveform</p> required <code>sampling_rate</code> <code>int</code> <p>Usually 44.100, but check the specifications of the TTS model you are using.</p> required <code>filename</code> <code>str</code> <p>the destination filename to save the audio</p> required Source code in <code>src/document_to_podcast/podcast_maker/script_to_audio.py</code> <pre><code>def save_waveform_as_file(\n    waveform: np.ndarray, sampling_rate: int, filename: str\n) -&gt; None:\n    \"\"\"\n    Save the output of the TTS (a numpy waveform) to a .wav file using the soundfile library.\n\n    Args:\n        waveform: 2D numpy array of a waveform\n        sampling_rate: Usually 44.100, but check the specifications of the TTS model you are using.\n        filename: the destination filename to save the audio\n\n    \"\"\"\n    sf.write(filename, waveform, sampling_rate)\n</code></pre>"},{"location":"api/#document_to_podcast.podcast_maker.config","title":"<code>document_to_podcast.podcast_maker.config</code>","text":""},{"location":"api/#document_to_podcast.podcast_maker.config.PodcastConfig","title":"<code>PodcastConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model that stores configuration of all the speakers for the TTS model. This allows different speakers to use different models and configurations.</p> Source code in <code>src/document_to_podcast/podcast_maker/config.py</code> <pre><code>class PodcastConfig(BaseModel):\n    \"\"\"\n    Pydantic model that stores configuration of all the speakers for the TTS model. This allows different speakers to\n    use different models and configurations.\n    \"\"\"\n\n    speakers: Dict[str, SpeakerConfig]\n    sampling_rate: int = 44_100\n</code></pre>"},{"location":"api/#document_to_podcast.podcast_maker.config.SpeakerConfig","title":"<code>SpeakerConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model that stores configuration of an individual speaker for the TTS model.</p> Source code in <code>src/document_to_podcast/podcast_maker/config.py</code> <pre><code>class SpeakerConfig(BaseModel):\n    \"\"\"\n    Pydantic model that stores configuration of an individual speaker for the TTS model.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    model: PreTrainedModel\n    speaker_id: str\n    # ParlerTTS specific configuration\n    tokenizer: Optional[PreTrainedTokenizerBase] = None\n    speaker_description: Optional[str] = (\n        None  # This description is used by the ParlerTTS model to configure the speaker profile\n    )\n</code></pre>"},{"location":"customization/","title":"\ud83c\udfa8 Customization Guide","text":"<p>The Document-to-Podcast Blueprint is designed to be flexible and easily adaptable to your specific needs. This guide will walk you through some key areas you can customize to make the Blueprint your own.</p>"},{"location":"customization/#changing-the-text-to-text-model","title":"\ud83e\udde0 Changing the Text-to-Text Model","text":"<p>You can swap the language model used for generating podcast scripts to suit your needs, such as using a smaller model for faster processing or a larger one for higher quality outputs.</p> <p>Customizing the app:</p> <ol> <li>Open the <code>app.py</code> file.</li> <li>Locate the <code>load_text_to_text_model</code> function.</li> <li>Replace the <code>model_id</code> with the ID of your desired model from a supported repository (e.g., Hugging Face). Note: The model repository must be in GGFUF format, for example: <code>Qwen/Qwen2.5-1.5B-Instruct-GGUF</code></li> </ol> <p>Example:</p> <pre><code>@st.cache_resource\ndef load_text_to_text_model():\n    return load_llama_cpp_model(\n        model_id=\"Qwen/Qwen2.5-1.5B-Instruct-GGUF/qwen2.5-1.5b-instruct-q8_0.gguf\"\n</code></pre>"},{"location":"customization/#modifying-the-text-generation-prompt","title":"\ud83d\udcdd Modifying the Text Generation Prompt","text":"<p>The system prompt defines the structure and tone of the generated script. Customizing this can allow you to generate conversations that align with your project\u2019s needs.</p> <p>Customizing the app:</p> <ol> <li>Open the <code>app.py</code> file.</li> <li>Locate the PODCAST_PROMPT variable.</li> <li>Edit the instructions to suit your desired conversation style.</li> </ol> <p>Example:</p> <pre><code>PODCAST_PROMPT = \"\"\"\nYou are a radio show scriptwriter generating lively and humorous dialogues.\nSpeaker 1: A comedian who is interested in learning new things.\nSpeaker 2: A scientist explaining concepts in a fun way.\n\"\"\"\n</code></pre>"},{"location":"customization/#customizing-speaker-descriptions","title":"\ud83c\udf99\ufe0f Customizing Speaker Descriptions","text":"<p>Adjusting the speaker profiles allows you to create distinct and engaging voices for your podcast.</p> <p>Customizing the app:</p> <ol> <li>Open the <code>app.py</code> file.</li> <li>Locate the SPEAKER_DESCRIPTIONS dictionary.</li> <li>Update the descriptions to define new voice characteristics for each speaker Example:</li> </ol> <pre><code>PODCAST_PROMPT = \"\"\"\nSPEAKER_DESCRIPTIONS = {\n    \"1\": \"A cheerful and animated voice with a fast-paced delivery.\",\n    \"2\": \"A calm and deep voice, speaking with authority and warmth.\"\n}\n\"\"\"\n</code></pre>"},{"location":"customization/#changing-the-text-to-speech-model","title":"\ud83e\udde0 Changing the Text-to-Speech Model","text":"<p>You can use a different TTS model to achieve specific voice styles or improve performance.</p> <p>Customizing the app:</p> <ol> <li>Open the <code>app.py</code> file.</li> <li>Locate the <code>load_text_to_speech_model_and_tokenizer</code> function.</li> <li>Replace the model_id with your preferred TTS model.</li> </ol> <p>Example: ```python @st.cache_resource def load_text_to_speech_model_and_tokenizer():     return load_parler_tts_model_and_tokenizer(         \"parler-tts/parler-tts-mini-expresso\", \"cpu\")</p>"},{"location":"customization/#other-customization-ideas","title":"\ud83d\udca1 Other Customization Ideas","text":"<ul> <li>Add Multiple Speakers: Modify <code>script_to_audio.py</code> to include additional speakers in your podcast.</li> </ul>"},{"location":"customization/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"future-features-contributions/","title":"\ud83d\ude80 Future Features &amp; Contributions","text":"<p>The Document-to-Podcast Blueprint is an evolving project designed to grow with the help of the open-source community. Whether you\u2019re an experienced developer or just starting, there are many ways you can contribute and help shape the future of this tool.</p>"},{"location":"future-features-contributions/#this-page-is-evolving","title":"\ud83d\udee0\ufe0f This Page is Evolving","text":"<p>As the community grows, we\u2019ll use this space to highlight contributions, showcase new ideas, and share guidance on expanding the Blueprint ecosystem.</p> <p>We have some ideas of how this Blueprint can be extended and improved, will be sharing these ideas and request for contributions shortly.</p>"},{"location":"future-features-contributions/#how-you-can-contribute","title":"\ud83c\udf1f How You Can Contribute","text":""},{"location":"future-features-contributions/#share-your-ideas","title":"\ud83d\udca1 Share Your Ideas","text":"<p>Got a vision for how this Blueprint could be improved? Share your suggestions through GitHub Discussions. Your insights can help inspire new directions for the project.</p>"},{"location":"future-features-contributions/#enhance-the-code","title":"\ud83d\udee0\ufe0f Enhance the Code","text":"<p>Dive into the codebase and contribute enhancements, optimizations, or bug fixes. Whether it's a small tweak or a big feature, every contribution helps! Start by checking our Contribution Guide (coming soon).</p>"},{"location":"future-features-contributions/#build-new-blueprints","title":"\ud83c\udf0d Build New Blueprints","text":"<p>This project is part of a larger initiative to create a collection of reusable starter code solutions that use open-source AI tools. If you\u2019re inspired to create your own Blueprint, we\u2019d love to see it!</p>"},{"location":"future-features-contributions/#get-involved","title":"\ud83e\udd1d Get Involved","text":"<ul> <li>Visit our GitHub Discussions to explore ongoing conversations and share your thoughts.</li> </ul> <p>Your contributions help make this Blueprint better for everyone. Thank you for being part of the journey! \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get started with Document-to-Podcast using one of the two options below: GitHub Codespaces for a hassle-free setup or Local Installation for running on your own machine.</p>"},{"location":"getting-started/#option-1-github-codespaces","title":"\u2601\ufe0f Option 1: GitHub Codespaces","text":"<p>The fastest way to get started. Click the button below to launch the project directly in GitHub Codespaces:</p> <p></p> <p>Once the Codespaces environment launches, inside the terminal, start the Streamlit demo by running: <pre><code>python -m streamlit run demo/app.py\n</code></pre></p>"},{"location":"getting-started/#option-2-local-installation","title":"\ud83d\udcbb  Option 2: Local Installation","text":"<ol> <li>Clone the Repository</li> </ol> <p>Inside your terminal, run:</p> <pre><code>git clone https://github.com/mozilla-ai/document-to-podcast.git\ncd document-to-podcast\n</code></pre> <ol> <li>Install Dependencies</li> </ol> <p>Inside your terminal, run:</p> <pre><code>pip install -e .\n</code></pre> <ol> <li>Run the Demo</li> </ol> <p>Inside your terminal, start the Streamlit demo by running:</p> <pre><code>python -m streamlit run demo/app.py\n</code></pre>"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide: How the Document-to-Podcast Blueprint Works","text":"<p>Transforming static documents into engaging podcast episodes involves an integration of pre-processing, LLM-powered transcript generation, and text-to-speech generation. Here's how it all works under the hood:</p>"},{"location":"step-by-step-guide/#overview","title":"Overview","text":"<p>This system has three core stages:</p> <p>\ud83d\udcc4 1. Document Pre-Processing    Prepare the input document by extracting and cleaning the text.</p> <p>\ud83d\udcdc 2. Podcast Script Generation    Use an LLM to transform the cleaned text into a conversational podcast script.</p> <p>\ud83c\udf99\ufe0f 3. Audio Podcast Generation    Convert the script into an engaging audio podcast with distinct speaker voices.</p> <p>We'll also look at how <code>app.py</code> brings all these steps together to build an end-to-end demo application.</p> <p>First, let\u2019s dive into each step to understand how this works in practice.</p>"},{"location":"step-by-step-guide/#step-1-document-pre-processing","title":"Step 1: Document Pre-Processing","text":"<p>The process begins with preparing the input document for AI processing. The system handles various document types while ensuring the extracted content is clean and structured.</p> <p>Cleaner input data ensures that the model works with reliable and consistent information, reducing the likelihood of confusing with unexpected tokens and therefore helping it to generate better outputs.</p>"},{"location":"step-by-step-guide/#key-components-in-this-doc-pre-processing","title":"\u2699\ufe0f Key Components in this Doc Pre-Processing","text":"<p>1 - File Loading</p> <ul> <li> <p>Uses functions defined in <code>data_loaders.py</code></p> </li> <li> <p>Supports <code>.html</code>, <code>.pdf</code>, <code>.txt</code>, and <code>.docx</code> formats.</p> </li> <li> <p>Extracts readable text from uploaded files using specialized loaders.</p> </li> </ul> <p>2 - Text Cleaning</p> <ul> <li> <p>Uses functions defined in <code>data_cleaners.py</code></p> </li> <li> <p>Removes unwanted elements like URLs, email addresses, and special characters using Python's <code>re</code> library, which leverages Regular Expressions (regex) to identify and manipulate specific patterns in text.</p> </li> <li> <p>Ensures the document is clean and ready for the next step.</p> </li> </ul>"},{"location":"step-by-step-guide/#step-2-podcast-script-generation","title":"Step 2: Podcast Script Generation","text":"<p>In this step, the pre-processed text is transformed into a conversational podcast transcript. Using a Language Model, the system generates a dialogue that\u2019s both informative and engaging.</p>"},{"location":"step-by-step-guide/#key-components-in-script-generation","title":"\u2699\ufe0f Key Components in Script Generation","text":"<p>1 - Model Loading</p> <ul> <li> <p>The <code>model_loader.py</code> script is responsible for loading GGUF-type models using the <code>llama_cpp</code> library.</p> </li> <li> <p>The function <code>load_llama_cpp_model</code> takes a model ID in the format <code>{org}/{repo}/{filename}</code> and loads the specified model.</p> </li> <li> <p>This approach of using the <code>llama_cpp</code> library supports efficient CPU-based inference, making language models accessible even on machines without GPUs.</p> </li> </ul> <p>2 - Text-to-Text Generation</p> <ul> <li> <p>The <code>text_to_text.py</code> script manages the interaction with the language model, converting input text into a structured conversational podcast script.</p> </li> <li> <p>It uses the <code>chat_completion</code> function to process the input text and a customizable system prompt, guiding the language to generate a text output (e.g. a coherent podcast script between speakers).</p> </li> <li> <p>The <code>return_json</code> parameter allows the output to be formatted as a JSON object style, which can make it easier to parse and integrate structured responses into applications.</p> </li> <li> <p>Supports both single-pass outputs (<code>text_to_text</code>) and real-time streamed responses (<code>text_to_text_stream</code>), offering flexibility for different use cases.</p> </li> </ul>"},{"location":"step-by-step-guide/#step-3-audio-podcast-generation","title":"Step 3: Audio Podcast Generation","text":"<p>In this final step, the generated podcast transcript is brought to life as an audio file. Using a Text-to-Speech (TTS) model, each speaker in the script is assigned a unique voice, creating an engaging and professional-sounding podcast.</p>"},{"location":"step-by-step-guide/#key-components-in-this-step","title":"\u2699\ufe0f Key Components in this Step","text":"<p>1 - Text-to-Speech Audio Generation</p> <ul> <li> <p>The <code>text_to_speech.py</code> script converts text into audio using a specified TTS model and tokenizer.</p> </li> <li> <p>A speaker profile defines the voice characteristics (e.g., tone, speed, clarity) for each speaker.</p> </li> <li> <p>The function <code>text_to_speech</code> takes the input text (e.g podcast script) and speaker profile, generating a waveform (audio data) that represents the spoken version of the text.</p> </li> </ul> <p>2 - Parsing and Combining Voices</p> <ul> <li> <p>The <code>script_to_audio.py</code> script ensures each speaker\u2019s dialogue is spoken in their unique voice.</p> </li> <li> <p>The function <code>parse_script_to_waveform</code> splits the dialogue script by speakers and uses <code>text_to_speech</code> to generate audio for each speaker, stitching them together into a full podcast.</p> </li> <li> <p>Once the podcast waveform is ready, the save_waveform_as_file function saves it as an audio file (e.g., MP3 or WAV), making it ready for distribution.</p> </li> </ul>"},{"location":"step-by-step-guide/#bringing-it-all-together-in-apppy","title":"Bringing It All Together in <code>app.py</code>","text":"<p>The <code>app.py</code> demo app is shows you how all the components of the Document-to-Podcast Blueprint can come together. It demonstrates how you can take the individual steps\u2014Document Pre-Processing, Podcast Script Generation, and Audio Podcast Generation\u2014and integrate them into a functional application. This is the heart of the Blueprint in action, showing how you can build an app using the provided tools and components.</p> <p>This demo uses Streamlit, an open-source Python framework for interactive apps.</p>"},{"location":"step-by-step-guide/#how-apppy-applies-each-step","title":"\ud83e\udde0 How <code>app.py</code> Applies Each Step","text":"<p>\ud83d\udcc4 Document Upload &amp; Pre-Processing</p> <ul> <li> <p>Users upload a file via the Streamlit interface (<code>st.file_uploader</code>), which supports <code>.pdf</code>, <code>.txt</code>, <code>.docx</code>, <code>.html</code>, and <code>.md</code> formats.</p> </li> <li> <p>The uploaded file is passed to the File Loading and Text Cleaning modules.</p> </li> <li> <p>Raw text is extracted using <code>DATA_LOADERS</code>, and the cleaned version is displayed alongside it using <code>DATA_CLEANERS</code>, and displayed to the end user.</p> </li> </ul> <p>\u2699\ufe0f Loading Models</p> <ul> <li> <p>The script uses <code>load_llama_cpp_model</code> from <code>model_loader.py</code> to load the LLM for generating the podcast script.</p> </li> <li> <p>Similarly, <code>load_parler_tts_model_and_tokenizer</code> is used to prepare the TTS model and tokenizer for audio generation.</p> </li> <li> <p>These models are cached using <code>@st.cache_resource</code> to ensure fast and efficient reuse during app interactions.</p> </li> </ul> <p>\ud83d\udcdd Podcast Script Generation</p> <ul> <li> <p>The cleaned text and a system-defined podcast prompt are fed into the text_to_text_stream function.</p> </li> <li> <p>The <code>PODCAST_PROMPT</code> can be edited by the end-user to enable them to tailor their script results for their needs.</p> </li> <li> <p>The script is streamed back to the user in real-time, allowing them to see the generated conversation between speakers</p> </li> </ul> <p>\ud83c\udf99\ufe0f Podcast Generation</p> <ul> <li> <p>For each speaker in the podcast script, audio is generated using the <code>text_to_speech</code> function with distinct speaker profiles</p> </li> <li> <p>The <code>SPEAKER_DESCRIPTION</code> enables the user to edit the podcast speakers voices to fit their needs.</p> </li> <li> <p>The generated audio is displayed with a player so users can listen directly in the app.</p> </li> </ul>"},{"location":"step-by-step-guide/#customizing-the-blueprint","title":"\ud83c\udfa8 Customizing the Blueprint","text":"<p>To better understand how you can tailor this Blueprint to suit your specific needs, please visit the Customization Guide.</p>"},{"location":"step-by-step-guide/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"}]}