{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Document-to-Podcast Blueprint","text":"<p>Blueprints empower developers to easily integrate AI capabilities into their projects using open-source models and tools.</p> <p>These docs are your companion to mastering the Document-to-Podcast Blueprint\u2014a local-first approach for transforming documents into engaging podcasts.</p>"},{"location":"#built-with","title":"Built with","text":"<ul> <li>Python 3.10+</li> <li>Llama-cpp (text-to-text, i.e script generation)</li> <li>OuteAI / Parler_tts (text-to-speech, i.e audio generation)</li> <li>Streamlit (UI demo)</li> </ul>"},{"location":"#get-started-quickly","title":"\ud83d\ude80 Get Started Quickly","text":""},{"location":"#start-building-your-own-document-to-podcast-pipeline-in-minutes","title":"Start building your own Document-to-Podcast pipeline in minutes:","text":"<ul> <li>Getting Started: Quick setup and installation instructions.</li> </ul>"},{"location":"#understand-the-system","title":"\ud83d\udd0d Understand the System","text":""},{"location":"#dive-deeper-into-how-the-blueprint-works","title":"Dive deeper into how the Blueprint works:","text":"<ul> <li>Step-by-Step Guide: A detailed breakdown of the system\u2019s design and workflow.</li> <li>API Reference: Explore the technical details of the core modules.</li> </ul>"},{"location":"#make-it-yours","title":"\ud83c\udfa8 Make It Yours","text":""},{"location":"#customize-the-blueprint-to-fit-your-needs","title":"Customize the Blueprint to fit your needs:","text":"<ul> <li>Customization Guide: Tailor prompts, voices, and settings to create unique podcasts.</li> </ul>"},{"location":"#join-the-community","title":"\ud83c\udf1f Join the Community","text":""},{"location":"#help-shape-the-future-of-blueprints","title":"Help shape the future of Blueprints:","text":"<ul> <li>Future Features &amp; Contributions: Learn about exciting upcoming features and how to contribute to the project.</li> </ul> <p>Have more questions? Reach out to us on Discord and we'll see how we can help:</p>"},{"location":"#why-blueprints","title":"Why Blueprints?","text":"<p>Blueprints are more than starter code\u2014they\u2019re your gateway to building AI-powered solutions with confidence. With step-by-step guidance, modular design, and open-source tools, we make AI accessible for developers of all skill levels.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#document_to_podcast.preprocessing.data_cleaners","title":"<code>document_to_podcast.preprocessing.data_cleaners</code>","text":""},{"location":"api/#document_to_podcast.preprocessing.data_cleaners.clean_html","title":"<code>clean_html(text)</code>","text":"<p>Clean HTML text.</p> This function removes <ul> <li>scripts</li> <li>styles</li> <li>links</li> <li>meta tags</li> </ul> <p>In addition, it calls clean_with_regex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clean_html(\"&lt;html&gt;&lt;body&gt;&lt;p&gt;Hello,  world!  &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\")\n\"Hello, world!\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The HTML text to clean.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text.</p> Source code in <code>src/document_to_podcast/preprocessing/data_cleaners.py</code> <pre><code>def clean_html(text: str) -&gt; str:\n    \"\"\"Clean HTML text.\n\n    This function removes:\n        - scripts\n        - styles\n        - links\n        - meta tags\n\n    In addition, it calls [clean_with_regex][document_to_podcast.preprocessing.data_cleaners.clean_with_regex].\n\n    Examples:\n        &gt;&gt;&gt; clean_html(\"&lt;html&gt;&lt;body&gt;&lt;p&gt;Hello,  world!  &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\")\n        \"Hello, world!\"\n\n    Args:\n        text (str): The HTML text to clean.\n\n    Returns:\n        str: The cleaned text.\n    \"\"\"\n    soup = BeautifulSoup(text, \"html.parser\")\n    for tag in soup([\"script\", \"style\", \"link\", \"meta\"]):\n        tag.decompose()\n    text = soup.get_text()\n    return clean_with_regex(text)\n</code></pre>"},{"location":"api/#document_to_podcast.preprocessing.data_cleaners.clean_markdown","title":"<code>clean_markdown(text)</code>","text":"<p>Clean Markdown text.</p> This function removes <ul> <li>markdown images</li> </ul> <p>In addition, it calls clean_with_regex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clean_markdown('# Title   with image ![alt text](image.jpg \"Image Title\")')\n\"Title with image\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The Markdown text to clean.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text.</p> Source code in <code>src/document_to_podcast/preprocessing/data_cleaners.py</code> <pre><code>def clean_markdown(text: str) -&gt; str:\n    \"\"\"Clean Markdown text.\n\n    This function removes:\n        - markdown images\n\n    In addition, it calls [clean_with_regex][document_to_podcast.preprocessing.data_cleaners.clean_with_regex].\n\n    Examples:\n        &gt;&gt;&gt; clean_markdown('# Title   with image ![alt text](image.jpg \"Image Title\")')\n        \"Title with image\"\n\n    Args:\n        text (str): The Markdown text to clean.\n\n    Returns:\n        str: The cleaned text.\n    \"\"\"\n    text = re.sub(r'!\\[.*?\\]\\(.*?(\".*?\")?\\)', \"\", text)\n\n    return clean_with_regex(text)\n</code></pre>"},{"location":"api/#document_to_podcast.preprocessing.data_cleaners.clean_with_regex","title":"<code>clean_with_regex(text)</code>","text":"<p>Clean text using regular expressions.</p> This function removes <ul> <li>URLs</li> <li>emails</li> <li>special characters</li> <li>extra spaces</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clean_with_regex(\"\u00a0Hello,   world! http://example.com\")\n\"Hello, world!\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to clean.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text.</p> Source code in <code>src/document_to_podcast/preprocessing/data_cleaners.py</code> <pre><code>def clean_with_regex(text: str) -&gt; str:\n    \"\"\"\n    Clean text using regular expressions.\n\n    This function removes:\n        - URLs\n        - emails\n        - special characters\n        - extra spaces\n\n    Examples:\n        &gt;&gt;&gt; clean_with_regex(\"\\xa0Hello,   world! http://example.com\")\n        \"Hello, world!\"\n\n    Args:\n        text (str): The text to clean.\n\n    Returns:\n        str: The cleaned text.\n    \"\"\"\n    text = re.sub(\n        r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n        \"\",\n        text,\n    )\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\\.[\\w]+\", \"\", text)\n    text = re.sub(r'[^a-zA-Z0-9\\s.,!?;:\"\\']', \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n</code></pre>"},{"location":"api/#document_to_podcast.inference.model_loaders","title":"<code>document_to_podcast.inference.model_loaders</code>","text":""},{"location":"api/#document_to_podcast.inference.model_loaders.load_llama_cpp_model","title":"<code>load_llama_cpp_model(model_id)</code>","text":"<p>Loads the given model_id using Llama.from_pretrained.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = load_llama_cpp_model(\n    \"allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model id to load. Format is expected to be <code>{org}/{repo}/{filename}</code>.</p> required <p>Returns:</p> Name Type Description <code>Llama</code> <code>Llama</code> <p>The loaded model.</p> Source code in <code>src/document_to_podcast/inference/model_loaders.py</code> <pre><code>def load_llama_cpp_model(\n    model_id: str,\n) -&gt; Llama:\n    \"\"\"\n    Loads the given model_id using Llama.from_pretrained.\n\n    Examples:\n        &gt;&gt;&gt; model = load_llama_cpp_model(\n            \"allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf\")\n\n    Args:\n        model_id (str): The model id to load.\n            Format is expected to be `{org}/{repo}/{filename}`.\n\n    Returns:\n        Llama: The loaded model.\n    \"\"\"\n    org, repo, filename = model_id.split(\"/\")\n    model = Llama.from_pretrained(\n        repo_id=f\"{org}/{repo}\",\n        filename=filename,\n        # 0 means that the model limit will be used, instead of the default (512) or other hardcoded value\n        n_ctx=0,\n        verbose=False,\n    )\n    return model\n</code></pre>"},{"location":"api/#document_to_podcast.inference.model_loaders.load_outetts_model","title":"<code>load_outetts_model(model_id, language='en', device='cpu')</code>","text":"<p>Loads the given model_id using the OuteTTS interface. For more info: https://github.com/edwko/OuteTTS</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = load_outetts_model(\"OuteAI/OuteTTS-0.1-350M-GGUF/OuteTTS-0.1-350M-FP16.gguf\", \"en\", \"cpu\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model id to load. Format is expected to be <code>{org}/{repo}/{filename}</code>.</p> required <code>language</code> <code>str</code> <p>Supported languages in 0.2-500M: en, zh, ja, ko.</p> <code>'en'</code> <code>device</code> <code>str</code> <p>The device to load the model on, such as \"cuda:0\" or \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>PreTrainedModel</code> <code>InterfaceGGUF</code> <p>The loaded model.</p> Source code in <code>src/document_to_podcast/inference/model_loaders.py</code> <pre><code>def load_outetts_model(\n    model_id: str, language: str = \"en\", device: str = \"cpu\"\n) -&gt; InterfaceGGUF:\n    \"\"\"\n    Loads the given model_id using the OuteTTS interface. For more info: https://github.com/edwko/OuteTTS\n\n    Examples:\n        &gt;&gt;&gt; model = load_outetts_model(\"OuteAI/OuteTTS-0.1-350M-GGUF/OuteTTS-0.1-350M-FP16.gguf\", \"en\", \"cpu\")\n\n    Args:\n        model_id (str): The model id to load.\n            Format is expected to be `{org}/{repo}/{filename}`.\n        language (str): Supported languages in 0.2-500M: en, zh, ja, ko.\n        device (str): The device to load the model on, such as \"cuda:0\" or \"cpu\".\n\n    Returns:\n        PreTrainedModel: The loaded model.\n    \"\"\"\n    n_layers_on_gpu = 0 if device == \"cpu\" else -1\n    model_version = model_id.split(\"-\")[1]\n\n    org, repo, filename = model_id.split(\"/\")\n    local_path = hf_hub_download(repo_id=f\"{org}/{repo}\", filename=filename)\n    model_config = GGUFModelConfig_v1(\n        model_path=local_path, language=language, n_gpu_layers=n_layers_on_gpu\n    )\n\n    return InterfaceGGUF(model_version=model_version, cfg=model_config)\n</code></pre>"},{"location":"api/#document_to_podcast.inference.model_loaders.load_parler_tts_model_and_tokenizer","title":"<code>load_parler_tts_model_and_tokenizer(model_id, device='cpu')</code>","text":"<p>Loads the given model_id using parler_tts.from_pretrained. For more info: https://github.com/huggingface/parler-tts</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model, tokenizer = load_parler_tts_model_and_tokenizer(\"parler-tts/parler-tts-mini-v1\", \"cpu\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model id to load. Format is expected to be <code>{repo}/{filename}</code>.</p> required <code>device</code> <code>str</code> <p>The device to load the model on, such as \"cuda:0\" or \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>PreTrainedModel</code> <code>Tuple[PreTrainedModel, PreTrainedTokenizerBase]</code> <p>The loaded model.</p> Source code in <code>src/document_to_podcast/inference/model_loaders.py</code> <pre><code>def load_parler_tts_model_and_tokenizer(\n    model_id: str, device: str = \"cpu\"\n) -&gt; Tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n    \"\"\"\n    Loads the given model_id using parler_tts.from_pretrained. For more info: https://github.com/huggingface/parler-tts\n\n    Examples:\n        &gt;&gt;&gt; model, tokenizer = load_parler_tts_model_and_tokenizer(\"parler-tts/parler-tts-mini-v1\", \"cpu\")\n\n    Args:\n        model_id (str): The model id to load.\n            Format is expected to be `{repo}/{filename}`.\n        device (str): The device to load the model on, such as \"cuda:0\" or \"cpu\".\n\n    Returns:\n        PreTrainedModel: The loaded model.\n    \"\"\"\n    from parler_tts import ParlerTTSForConditionalGeneration\n\n    model = ParlerTTSForConditionalGeneration.from_pretrained(model_id).to(device)\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    return model, tokenizer\n</code></pre>"},{"location":"api/#document_to_podcast.inference.text_to_text","title":"<code>document_to_podcast.inference.text_to_text</code>","text":""},{"location":"api/#document_to_podcast.inference.text_to_text.text_to_text","title":"<code>text_to_text(input_text, model, system_prompt, return_json=True, stop=None)</code>","text":"<p>Transforms input_text using the given model and system prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to be transformed.</p> required <code>model</code> <code>Llama</code> <p>The model to use for conversion.</p> required <code>system_prompt</code> <code>str</code> <p>The system prompt to use for conversion.</p> required <code>return_json</code> <code>bool</code> <p>Whether to return the response as JSON. Defaults to True.</p> <code>True</code> <code>stop</code> <code>str | list[str] | None</code> <p>The stop token(s).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full transformed text.</p> Source code in <code>src/document_to_podcast/inference/text_to_text.py</code> <pre><code>def text_to_text(\n    input_text: str,\n    model: Llama,\n    system_prompt: str,\n    return_json: bool = True,\n    stop: str | list[str] | None = None,\n) -&gt; str:\n    \"\"\"\n    Transforms input_text using the given model and system prompt.\n\n    Args:\n        input_text (str): The text to be transformed.\n        model (Llama): The model to use for conversion.\n        system_prompt (str): The system prompt to use for conversion.\n        return_json (bool, optional): Whether to return the response as JSON.\n            Defaults to True.\n        stop (str | list[str] | None, optional): The stop token(s).\n\n    Returns:\n        str: The full transformed text.\n    \"\"\"\n    response = chat_completion(\n        input_text, model, system_prompt, return_json, stop=stop, stream=False\n    )\n    return response[\"choices\"][0][\"message\"][\"content\"]\n</code></pre>"},{"location":"api/#document_to_podcast.inference.text_to_text.text_to_text_stream","title":"<code>text_to_text_stream(input_text, model, system_prompt, return_json=True, stop=None)</code>","text":"<p>Transforms input_text using the given model and system prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to be transformed.</p> required <code>model</code> <code>Llama</code> <p>The model to use for conversion.</p> required <code>system_prompt</code> <code>str</code> <p>The system prompt to use for conversion.</p> required <code>return_json</code> <code>bool</code> <p>Whether to return the response as JSON. Defaults to True.</p> <code>True</code> <code>stop</code> <code>str | list[str] | None</code> <p>The stop token(s).</p> <code>None</code> <p>Yields:</p> Name Type Description <code>str</code> <code>str</code> <p>Chunks of the transformed text as they are available.</p> Source code in <code>src/document_to_podcast/inference/text_to_text.py</code> <pre><code>def text_to_text_stream(\n    input_text: str,\n    model: Llama,\n    system_prompt: str,\n    return_json: bool = True,\n    stop: str | list[str] | None = None,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Transforms input_text using the given model and system prompt.\n\n    Args:\n        input_text (str): The text to be transformed.\n        model (Llama): The model to use for conversion.\n        system_prompt (str): The system prompt to use for conversion.\n        return_json (bool, optional): Whether to return the response as JSON.\n            Defaults to True.\n        stop (str | list[str] | None, optional): The stop token(s).\n\n    Yields:\n        str: Chunks of the transformed text as they are available.\n    \"\"\"\n    response = chat_completion(\n        input_text, model, system_prompt, return_json, stop=stop, stream=True\n    )\n    for item in response:\n        if item[\"choices\"][0].get(\"delta\", {}).get(\"content\", None):\n            yield item[\"choices\"][0].get(\"delta\", {}).get(\"content\", None)\n</code></pre>"},{"location":"api/#document_to_podcast.inference.text_to_speech","title":"<code>document_to_podcast.inference.text_to_speech</code>","text":""},{"location":"api/#document_to_podcast.inference.text_to_speech.text_to_speech","title":"<code>text_to_speech(input_text, model, voice_profile, tokenizer=None)</code>","text":"<p>Generates a speech waveform from a text input using a pre-trained text-to-speech (TTS) model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; waveform = text_to_speech(input_text=\"Welcome to our amazing podcast\", model=model, voice_profile=\"male_1\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to convert to speech.</p> required <code>model</code> <code>PreTrainedModel</code> <p>The model used for generating the waveform.</p> required <code>voice_profile</code> <code>str</code> <p>Depending on the selected TTS model it should either be - a pre-defined ID for the Oute models (e.g. \"female_1\") more info here https://github.com/edwko/OuteTTS/tree/main/outetts/version/v1/default_speakers - a natural description of the voice profile using a pre-defined name for the Parler model (e.g. Laura's voice is calm) more info here https://github.com/huggingface/parler-tts?tab=readme-ov-file#-using-a-specific-speaker</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>[Only used for the Parler models!] The tokenizer used for tokenizing the text in order to send to the model.</p> <code>None</code> <p>Returns:     numpy array: The waveform of the speech as a 2D numpy array</p> Source code in <code>src/document_to_podcast/inference/text_to_speech.py</code> <pre><code>def text_to_speech(\n    input_text: str,\n    model: Union[InterfaceGGUF, PreTrainedModel],\n    voice_profile: str,\n    tokenizer: PreTrainedTokenizerBase = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generates a speech waveform from a text input using a pre-trained text-to-speech (TTS) model.\n\n    Examples:\n        &gt;&gt;&gt; waveform = text_to_speech(input_text=\"Welcome to our amazing podcast\", model=model, voice_profile=\"male_1\")\n\n    Args:\n        input_text (str): The text to convert to speech.\n        model (PreTrainedModel): The model used for generating the waveform.\n        voice_profile (str): Depending on the selected TTS model it should either be\n            - a pre-defined ID for the Oute models (e.g. \"female_1\")\n            more info here https://github.com/edwko/OuteTTS/tree/main/outetts/version/v1/default_speakers\n            - a natural description of the voice profile using a pre-defined name for the Parler model (e.g. Laura's voice is calm)\n            more info here https://github.com/huggingface/parler-tts?tab=readme-ov-file#-using-a-specific-speaker\n        tokenizer (PreTrainedTokenizerBase): [Only used for the Parler models!] The tokenizer used for tokenizing the text in order to send to the model.\n    Returns:\n        numpy array: The waveform of the speech as a 2D numpy array\n    \"\"\"\n    if isinstance(model, InterfaceGGUF):\n        return _text_to_speech_oute(input_text, model, voice_profile)\n    elif isinstance(model, PreTrainedModel):\n        return _text_to_speech_parler(input_text, model, tokenizer, voice_profile)\n    else:\n        raise NotImplementedError(\"Model not yet implemented for TTS\")\n</code></pre>"},{"location":"cli/","title":"Command Line Interface","text":"<p>Once you have installed the blueprint, you can use it from the CLI.</p> <p>You can either provide the path to a configuration file:</p> <pre><code>document-to-podcast --from_config \"example_data/config.yaml\"\n</code></pre> <p>Or provide values to the arguments directly:</p> <pre><code>document-to-podcast \\\n--input_file \"example_data/Mozilla-Trustworthy_AI.pdf\" \\\n--output_folder \"example_data\"\n--text_to_text_model \"Qwen/Qwen2.5-1.5B-Instruct-GGUF/qwen2.5-1.5b-instruct-q8_0.gguf\"\n</code></pre>"},{"location":"cli/#document_to_podcast.cli.document_to_podcast","title":"<code>document_to_podcast.cli.document_to_podcast(input_file=None, output_folder=None, text_to_text_model='allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf', text_to_text_prompt=DEFAULT_PROMPT, text_to_speech_model='OuteAI/OuteTTS-0.1-350M-GGUF/OuteTTS-0.1-350M-FP16.gguf', speakers=None, from_config=None)</code>","text":"<p>Generate a podcast from a document.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The path to the input file. Supported extensions:</p> <pre><code>- .pdf\n- .html\n- .txt\n- .docx\n- .md\n</code></pre> <code>None</code> <code>output_folder</code> <code>str</code> <p>The path to the output folder. Two files will be created:</p> <pre><code>- {output_folder}/podcast.txt\n- {output_folder}/podcast.wav\n</code></pre> <code>None</code> <code>text_to_text_model</code> <code>str</code> <p>The path to the text-to-text model.</p> <p>Need to be formatted as <code>owner/repo/file</code>.</p> <p>Need to be a gguf file.</p> <p>Defaults to <code>allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf</code>.</p> <code>'allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf'</code> <code>text_to_text_prompt</code> <code>str</code> <p>The prompt for the text-to-text model. Defaults to DEFAULT_PROMPT.</p> <code>DEFAULT_PROMPT</code> <code>text_to_speech_model</code> <code>str</code> <p>The path to the text-to-speech model. Defaults to <code>OuteAI/OuteTTS-0.1-350M-GGUF/OuteTTS-0.1-350M-FP16.gguf</code>.</p> <code>'OuteAI/OuteTTS-0.1-350M-GGUF/OuteTTS-0.1-350M-FP16.gguf'</code> <code>speakers</code> <code>list[Speaker] | None</code> <p>The speakers for the podcast. Defaults to DEFAULT_SPEAKERS.</p> <code>None</code> <code>from_config</code> <code>str</code> <p>The path to the config file. Defaults to None.</p> <p>If provided, all other arguments will be ignored.</p> <code>None</code> Source code in <code>src/document_to_podcast/cli.py</code> <pre><code>@logger.catch(reraise=True)\ndef document_to_podcast(\n    input_file: str | None = None,\n    output_folder: str | None = None,\n    text_to_text_model: str = \"allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf\",\n    text_to_text_prompt: str = DEFAULT_PROMPT,\n    text_to_speech_model: SUPPORTED_TTS_MODELS = \"OuteAI/OuteTTS-0.1-350M-GGUF/OuteTTS-0.1-350M-FP16.gguf\",\n    speakers: list[Speaker] | None = None,\n    from_config: str | None = None,\n):\n    \"\"\"\n    Generate a podcast from a document.\n\n    Args:\n        input_file (str): The path to the input file.\n            Supported extensions:\n\n                - .pdf\n                - .html\n                - .txt\n                - .docx\n                - .md\n\n        output_folder (str): The path to the output folder.\n            Two files will be created:\n\n                - {output_folder}/podcast.txt\n                - {output_folder}/podcast.wav\n\n        text_to_text_model (str, optional): The path to the text-to-text model.\n\n            Need to be formatted as `owner/repo/file`.\n\n            Need to be a gguf file.\n\n            Defaults to `allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf`.\n\n        text_to_text_prompt (str, optional): The prompt for the text-to-text model.\n            Defaults to DEFAULT_PROMPT.\n\n        text_to_speech_model (str, optional): The path to the text-to-speech model.\n            Defaults to `OuteAI/OuteTTS-0.1-350M-GGUF/OuteTTS-0.1-350M-FP16.gguf`.\n\n        speakers (list[Speaker] | None, optional): The speakers for the podcast.\n            Defaults to DEFAULT_SPEAKERS.\n\n        from_config (str, optional): The path to the config file. Defaults to None.\n\n\n            If provided, all other arguments will be ignored.\n    \"\"\"\n    if from_config:\n        config = Config.model_validate(yaml.safe_load(Path(from_config).read_text()))\n    else:\n        speakers = speakers or DEFAULT_SPEAKERS\n        config = Config(\n            input_file=input_file,\n            output_folder=output_folder,\n            text_to_text_model=text_to_text_model,\n            text_to_text_prompt=text_to_text_prompt,\n            text_to_speech_model=text_to_speech_model,\n            speakers=[Speaker.model_validate(speaker) for speaker in speakers],\n        )\n\n    output_folder = Path(config.output_folder)\n    output_folder.mkdir(parents=True, exist_ok=True)\n\n    data_loader = DATA_LOADERS[Path(config.input_file).suffix]\n    logger.info(f\"Loading {config.input_file}\")\n    raw_text = data_loader(config.input_file)\n    logger.debug(f\"Loaded {len(raw_text)} characters\")\n\n    data_cleaner = DATA_CLEANERS[Path(config.input_file).suffix]\n    logger.info(f\"Cleaning {config.input_file}\")\n    clean_text = data_cleaner(raw_text)\n    logger.debug(f\"Cleaned {len(raw_text) - len(clean_text)} characters\")\n    logger.debug(f\"Length of cleaned text: {len(clean_text)}\")\n\n    logger.info(f\"Loading {config.text_to_text_model}\")\n    text_model = load_llama_cpp_model(model_id=config.text_to_text_model)\n\n    logger.info(f\"Loading {config.text_to_speech_model}\")\n    if \"oute\" in config.text_to_speech_model.lower():\n        speech_model = load_outetts_model(model_id=config.text_to_speech_model)\n        speech_tokenizer = None\n        sample_rate = speech_model.audio_codec.sr\n    else:\n        speech_model, speech_tokenizer = load_parler_tts_model_and_tokenizer(\n            model_id=config.text_to_speech_model\n        )\n        sample_rate = speech_model.config.sampling_rate\n\n    # ~4 characters per token is considered a reasonable default.\n    max_characters = text_model.n_ctx() * 4\n    if len(clean_text) &gt; max_characters:\n        logger.warning(\n            f\"Input text is too big ({len(clean_text)}).\"\n            f\" Using only a subset of it ({max_characters}).\"\n        )\n    clean_text = clean_text[:max_characters]\n\n    logger.info(\"Generating Podcast...\")\n    podcast_script = \"\"\n    text = \"\"\n    podcast_audio = []\n    system_prompt = config.text_to_text_prompt.strip()\n    system_prompt = system_prompt.replace(\n        \"{SPEAKERS}\", \"\\n\".join(str(speaker) for speaker in config.speakers)\n    )\n    for chunk in text_to_text_stream(\n        clean_text, text_model, system_prompt=system_prompt\n    ):\n        text += chunk\n        podcast_script += chunk\n        if text.endswith(\"\\n\") and \"Speaker\" in text:\n            logger.debug(text)\n            speaker_id = re.search(r\"Speaker (\\d+)\", text).group(1)\n            voice_profile = next(\n                speaker.voice_profile\n                for speaker in config.speakers\n                if speaker.id == int(speaker_id)\n            )\n            speech = text_to_speech(\n                text.split(f'\"Speaker {speaker_id}\":')[-1],\n                speech_model,\n                voice_profile,\n                tokenizer=speech_tokenizer,  # Applicable only for parler models\n            )\n            podcast_audio.append(speech)\n            text = \"\"\n\n    logger.info(\"Saving Podcast...\")\n    sf.write(\n        str(output_folder / \"podcast.wav\"),\n        np.concatenate(podcast_audio),\n        samplerate=sample_rate,\n    )\n    (output_folder / \"podcast.txt\").write_text(podcast_script)\n    logger.success(\"Done!\")\n</code></pre>"},{"location":"cli/#document_to_podcast.config.Config","title":"<code>document_to_podcast.config.Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/document_to_podcast/config.py</code> <pre><code>class Config(BaseModel):\n    input_file: Annotated[FilePath, AfterValidator(validate_input_file)]\n    output_folder: str\n    text_to_text_model: Annotated[str, AfterValidator(validate_text_to_text_model)]\n    text_to_text_prompt: Annotated[str, AfterValidator(validate_text_to_text_prompt)]\n    text_to_speech_model: SUPPORTED_TTS_MODELS\n    speakers: list[Speaker]\n</code></pre>"},{"location":"cli/#document_to_podcast.config.Speaker","title":"<code>document_to_podcast.config.Speaker</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/document_to_podcast/config.py</code> <pre><code>class Speaker(BaseModel):\n    id: int\n    name: str\n    description: str\n    voice_profile: str\n\n    def __str__(self):\n        return f\"Speaker {self.id}. Named {self.name}. {self.description}\"\n</code></pre>"},{"location":"cli/#document_to_podcast.config.DEFAULT_PROMPT","title":"<code>document_to_podcast.config.DEFAULT_PROMPT = '\\nYou are a podcast scriptwriter generating engaging and natural-sounding conversations in JSON format.\\nThe script features the following speakers:\\n{SPEAKERS}\\nInstructions:\\n- Write dynamic, easy-to-follow dialogue.\\n- Include natural interruptions and interjections.\\n- Avoid repetitive phrasing between speakers.\\n- Format output as a JSON conversation.\\nExample:\\n{\\n  \"Speaker 1\": \"Welcome to our podcast! Today, we\\'re exploring...\",\\n  \"Speaker 2\": \"Hi! I\\'m excited to hear about this. Can you explain...\",\\n  \"Speaker 1\": \"Sure! Imagine it like this...\",\\n  \"Speaker 2\": \"Oh, that\\'s cool! But how does...\"\\n}\\n'</code>  <code>module-attribute</code>","text":""},{"location":"cli/#document_to_podcast.config.DEFAULT_SPEAKERS","title":"<code>document_to_podcast.config.DEFAULT_SPEAKERS = [{'id': 1, 'name': 'Laura', 'description': 'The main host. She explains topics clearly using anecdotes and analogies, teaching in an engaging and captivating way.', 'voice_profile': 'female_1'}, {'id': 2, 'name': 'Jon', 'description': 'The co-host. He keeps the conversation on track, asks curious follow-up questions, and reacts with excitement or confusion, often using interjections like hmm or umm.', 'voice_profile': 'male_1'}]</code>  <code>module-attribute</code>","text":""},{"location":"customization/","title":"\ud83c\udfa8 Customization Guide","text":"<p>The Document-to-Podcast Blueprint is designed to be flexible and easily adaptable to your specific needs. This guide will walk you through some key areas you can customize to make the Blueprint your own.</p>"},{"location":"customization/#changing-the-text-to-text-model","title":"\ud83e\udde0 Changing the Text-to-Text Model","text":"<p>You can swap the language model used for generating podcast scripts to suit your needs, such as using a smaller model for faster processing or a larger one for higher quality outputs.</p> <p>Customizing the app:</p> <ol> <li>Open the <code>app.py</code> file.</li> <li>Locate the <code>load_text_to_text_model</code> function.</li> <li>Replace the <code>model_id</code> with the ID of your desired model from a supported repository (e.g., Hugging Face). Note: The model repository must be in GGFUF format, for example: <code>Qwen/Qwen2.5-1.5B-Instruct-GGUF</code></li> </ol> <p>Example:</p> <pre><code>@st.cache_resource\ndef load_text_to_text_model():\n    return load_llama_cpp_model(\n        model_id=\"Qwen/Qwen2.5-1.5B-Instruct-GGUF/qwen2.5-1.5b-instruct-q8_0.gguf\"\n</code></pre>"},{"location":"customization/#modifying-the-text-generation-prompt","title":"\ud83d\udcdd Modifying the Text Generation Prompt","text":"<p>The system prompt defines the structure and tone of the generated script. Customizing this can allow you to generate conversations that align with your project\u2019s needs.</p> <p>Customizing the app:</p> <ol> <li>Open the <code>app.py</code> file.</li> <li>Locate the PODCAST_PROMPT variable.</li> <li>Edit the instructions to suit your desired conversation style.</li> </ol> <p>Example:</p> <pre><code>PODCAST_PROMPT = \"\"\"\nYou are a radio show scriptwriter generating lively and humorous dialogues.\nSpeaker 1: A comedian who is interested in learning new things.\nSpeaker 2: A scientist explaining concepts in a fun way.\n\"\"\"\n</code></pre>"},{"location":"customization/#customizing-speaker-descriptions","title":"\ud83c\udf99\ufe0f Customizing Speaker Descriptions","text":"<p>Adjusting the speaker profiles allows you to create distinct and engaging voices for your podcast.</p> <p>Customizing the app:</p> <ol> <li>Open the <code>app.py</code> file.</li> <li>Locate the SPEAKER_DESCRIPTIONS dictionary.</li> <li>Update the descriptions to define new voice characteristics for each speaker Example:</li> </ol> <pre><code>SPEAKER_DESCRIPTIONS_OUTE = {\n    \"1\": \"A cheerful and animated voice with a fast-paced delivery.\",\n    \"2\": \"A calm and deep voice, speaking with authority and warmth.\"\n}\n\"\"\"\n</code></pre>"},{"location":"customization/#changing-the-text-to-speech-model","title":"\ud83e\udde0 Changing the Text-to-Speech Model","text":"<p>You can use a different TTS model to achieve specific voice styles or improve performance.</p> <p>Customizing the app:</p> <ol> <li>Open the <code>app.py</code> file.</li> <li>Locate the <code>load_text_to_speech_model_and_tokenizer</code> function.</li> <li>Replace the model_id with your preferred TTS model.</li> </ol> <p>Example: <pre><code>@st.cache_resource\ndef load_text_to_speech_model_and_tokenizer():\n    return load_parler_tts_model_and_tokenizer(\n        \"parler-tts/parler-tts-mini-expresso\", \"cpu\")\n</code></pre></p>"},{"location":"customization/#other-customization-ideas","title":"\ud83d\udca1 Other Customization Ideas","text":"<ul> <li>Add Multiple Speakers: Modify <code>script_to_audio.py</code> to include additional speakers in your podcast.</li> </ul>"},{"location":"customization/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"future-features-contributions/","title":"\ud83d\ude80 Future Features &amp; Contributions","text":"<p>The Document-to-Podcast Blueprint is an evolving project designed to grow with the help of the open-source community. Whether you\u2019re an experienced developer or just starting, there are many ways you can contribute and help shape the future of this tool.</p>"},{"location":"future-features-contributions/#how-you-can-contribute","title":"\ud83c\udf1f How You Can Contribute","text":""},{"location":"future-features-contributions/#enhance-the-blueprint","title":"\ud83d\udee0\ufe0f Enhance the Blueprint","text":"<ul> <li>Check the Issues page to see if there are feature requests you'd like to implement</li> <li>Refer to our Contribution Guide for more details on contributions</li> </ul>"},{"location":"future-features-contributions/#extensibility-ideas","title":"\ud83c\udfa8 Extensibility Ideas","text":"<p>This Blueprint is designed to be a foundation you can build upon. By extending its capabilities, you can open the door to new applications, improve user experience, and adapt the Blueprint to address other use cases. Here are a few ideas for how you can expand its potential:</p> <ul> <li>Multi-language podcast generation: Add support for multi-language podcast generation to expand the reach of this Blueprint.</li> <li>New modalities input: Add support to the Blueprint to be able to handle different input modalities, like audio or images, enabling more flexibility in podcast generation.</li> <li>Improved audio quality: Explore and integrate more advanced open-source TTS frameworks to enhance the quality of generated audio, making podcasts sound more natural.</li> </ul> <p>We\u2019d love to see how you can enhance this Blueprint! If you create improvements or extend its capabilities, consider contributing them back to the project so others in the community can benefit from your work. Check out our Contributions Guide to get started!</p>"},{"location":"future-features-contributions/#share-your-ideas","title":"\ud83d\udca1 Share Your Ideas","text":"<p>Got an idea for how this Blueprint could be improved? You can share your suggestions through GitHub Discussions.</p>"},{"location":"future-features-contributions/#build-new-blueprints","title":"\ud83c\udf0d Build New Blueprints","text":"<p>This project is part of a larger initiative to create a collection of reusable starter code solutions that use open-source AI tools. If you\u2019re inspired to create your own Blueprint, you can use the Blueprint-template to get started.</p> <p>Your contributions help make this Blueprint better for everyone \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get started with Document-to-Podcast using one of the two options below: GitHub Codespaces for a hassle-free setup or Local Installation for running on your own machine.</p>"},{"location":"getting-started/#option-1-github-codespaces","title":"\u2601\ufe0f Option 1: GitHub Codespaces","text":"<p>The fastest way to get started. Click the button below to launch the project directly in GitHub Codespaces:</p> <p></p> <p>Once the Codespaces environment launches, inside the terminal, start the Streamlit demo by running: <pre><code>python -m streamlit run demo/app.py\n</code></pre></p>"},{"location":"getting-started/#option-2-local-installation","title":"\ud83d\udcbb  Option 2: Local Installation","text":"<p>1.Clone the Repository</p> <p>Inside your terminal, run: <pre><code>   git clone https://github.com/mozilla-ai/document-to-podcast.git\n   cd document-to-podcast\n</code></pre> 2. Install Dependencies</p> <p>Inside your terminal, run:</p> <p><pre><code>pip install -e .\n</code></pre> 3. Run the Demo</p> <p>Inside your terminal, start the Streamlit demo by running:</p> <pre><code>python -m streamlit run demo/app.py\n</code></pre>"},{"location":"getting-started/#optional-use-parler-models-for-text-to-speech","title":"[Optional]: Use Parler models for text-to-speech","text":"<p>If you want to use the parler tts models, you will need to additionally install an optional dependency by running: <pre><code>pip install -e '.[parler]'\n</code></pre></p>"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide: How the Document-to-Podcast Blueprint Works","text":"<p>Transforming static documents into engaging podcast episodes involves an integration of pre-processing, LLM-powered transcript generation, and text-to-speech generation. Here's how it all works under the hood:</p>"},{"location":"step-by-step-guide/#overview","title":"Overview","text":"<p>This system has three core stages:</p> <p>\ud83d\udcc4 1. Document Pre-Processing    Prepare the input document by extracting and cleaning the text.</p> <p>\ud83d\udcdc 2. Podcast Script Generation    Use an LLM to transform the cleaned text into a conversational podcast script.</p> <p>\ud83c\udf99\ufe0f 3. Audio Podcast Generation    Convert the script into an engaging audio podcast with distinct speaker voices.</p> <p>We'll also look at how <code>app.py</code> brings all these steps together to build an end-to-end demo application.</p> <p>First, let\u2019s dive into each step to understand how this works in practice.</p>"},{"location":"step-by-step-guide/#step-1-document-pre-processing","title":"Step 1: Document Pre-Processing","text":"<p>The process begins with preparing the input document for AI processing. The system handles various document types while ensuring the extracted content is clean and structured.</p> <p>Cleaner input data ensures that the model works with reliable and consistent information, reducing the likelihood of confusing with unexpected tokens and therefore helping it to generate better outputs.</p>"},{"location":"step-by-step-guide/#key-components-in-this-step","title":"\u2699\ufe0f Key Components in this Step","text":"<p>1 - File Loading</p> <ul> <li> <p>Uses functions defined in <code>data_loaders.py</code></p> </li> <li> <p>Supports <code>.html</code>, <code>.pdf</code>, <code>.txt</code>, and <code>.docx</code> formats.</p> </li> <li> <p>Extracts readable text from uploaded files using specialized loaders.</p> </li> </ul> <p>2 - Text Cleaning</p> <ul> <li> <p>Uses functions defined in <code>data_cleaners.py</code></p> </li> <li> <p>Removes unwanted elements like URLs, email addresses, and special characters using Python's <code>re</code> library, which leverages Regular Expressions (regex) to identify and manipulate specific patterns in text.</p> </li> <li> <p>Ensures the document is clean and ready for the next step.</p> </li> </ul>"},{"location":"step-by-step-guide/#step-2-podcast-script-generation","title":"Step 2: Podcast Script Generation","text":"<p>In this step, the pre-processed text is transformed into a conversational podcast transcript. Using a Language Model, the system generates a dialogue that\u2019s both informative and engaging.</p>"},{"location":"step-by-step-guide/#key-components-in-this-step_1","title":"\u2699\ufe0f Key Components in this Step","text":"<p>1 - Model Loading</p> <ul> <li> <p>The <code>model_loader.py</code> module is responsible for loading the <code>text-to-text</code> models using the <code>llama_cpp</code> library.</p> </li> <li> <p>The function <code>load_llama_cpp_model</code> takes a model ID in the format <code>{org}/{repo}/{filename}</code> and loads the specified model. This approach of using the <code>llama_cpp</code> library supports efficient CPU-based inference, making language models accessible even on machines without GPUs.</p> </li> </ul> <p>2 - Text-to-Text Generation</p> <ul> <li> <p>The <code>text_to_text.py</code> script manages the interaction with the language model, converting input text into a structured conversational podcast script.</p> </li> <li> <p>It uses the <code>chat_completion</code> function to process the input text and a customizable system prompt, guiding the language to generate a text output (e.g. a coherent podcast script between speakers).</p> </li> <li> <p>The <code>return_json</code> parameter allows the output to be formatted as a JSON object style, which can make it easier to parse and integrate structured responses into applications.</p> </li> <li> <p>Supports both single-pass outputs (<code>text_to_text</code>) and real-time streamed responses (<code>text_to_text_stream</code>), offering flexibility for different use cases.</p> </li> </ul>"},{"location":"step-by-step-guide/#step-3-audio-podcast-generation","title":"Step 3: Audio Podcast Generation","text":"<p>In this final step, the generated podcast transcript is brought to life as an audio file. Using a Text-to-Speech (TTS) model, each speaker in the script is assigned a unique voice, creating an engaging and professional-sounding podcast.</p>"},{"location":"step-by-step-guide/#key-components-in-this-step_2","title":"\u2699\ufe0f Key Components in this Step","text":"<p>1 - Model Loading</p> <ul> <li> <p>The <code>model_loader.py</code> module is responsible for loading the <code>text-to-speech</code> models using the <code>outetts</code> and <code>parler_tts</code> libraries.</p> </li> <li> <p>The function <code>load_outetts_model</code> takes a model ID in the format <code>{org}/{repo}/{filename}</code> and loads the specified model, either on CPU or GPU, based on the <code>device</code> parameter. The parameter <code>language</code> also enables to swap between the languages the Oute package supports (as of Dec 2024: <code>en, zh, ja, ko</code>)</p> </li> <li> <p>The function <code>load_parler_tts_model_and_tokenizer</code> takes a model ID in the format <code>{repo}/{filename}</code> and loads the specified model and tokenizer, either on CPU or GPU, based on the <code>device</code> parameter.</p> </li> </ul> <p>2 - Text-to-Speech Audio Generation</p> <ul> <li> <p>The <code>text_to_speech.py</code> script converts text into audio using a specified TTS model.</p> </li> <li> <p>A speaker profile defines the voice characteristics (e.g., tone, speed, clarity) for each speaker. This is specific to each TTS package. Oute models require one of the IDs specified here. Parler requires natural language description of the speaker's voice and you have to use a pre-defined name (see here)</p> </li> <li> <p>The function <code>text_to_speech</code> takes the input text (e.g. podcast script) and speaker profile, generating a waveform (audio data in a numpy array) that represents the spoken version of the text.</p> </li> </ul>"},{"location":"step-by-step-guide/#bringing-it-all-together-in-apppy","title":"Bringing It All Together in <code>app.py</code>","text":"<p>The <code>app.py</code> demo app is shows you how all the components of the Document-to-Podcast Blueprint can come together. It demonstrates how you can take the individual steps\u2014Document Pre-Processing, Podcast Script Generation, and Audio Podcast Generation\u2014and integrate them into a functional application. This is the heart of the Blueprint in action, showing how you can build an app using the provided tools and components.</p> <p>This demo uses Streamlit, an open-source Python framework for interactive apps.</p>"},{"location":"step-by-step-guide/#how-apppy-applies-each-step","title":"\ud83e\udde0 How <code>app.py</code> Applies Each Step","text":"<p>\ud83d\udcc4 Document Upload &amp; Pre-Processing</p> <ul> <li> <p>Users upload a file via the Streamlit interface (<code>st.file_uploader</code>), which supports <code>.pdf</code>, <code>.txt</code>, <code>.docx</code>, <code>.html</code>, and <code>.md</code> formats.</p> </li> <li> <p>The uploaded file is passed to the File Loading and Text Cleaning modules.</p> </li> <li> <p>Raw text is extracted using <code>DATA_LOADERS</code>, and the cleaned version is displayed alongside it using <code>DATA_CLEANERS</code>, and displayed to the end user.</p> </li> </ul> <p>\u2699\ufe0f Loading Models</p> <ul> <li> <p>The script uses <code>load_llama_cpp_model</code> from <code>model_loader.py</code> to load the LLM for generating the podcast script.</p> </li> <li> <p>Similarly, <code>load_outetts_model</code> is used to prepare the TTS model and tokenizer for audio generation.</p> </li> <li> <p>These models are cached using <code>@st.cache_resource</code> to ensure fast and efficient reuse during app interactions.</p> </li> </ul> <p>\ud83d\udcdd Podcast Script Generation</p> <ul> <li> <p>The cleaned text and a system-defined podcast prompt are fed into the text_to_text_stream function.</p> </li> <li> <p>The <code>PODCAST_PROMPT</code> can be edited by the end-user to enable them to tailor their script results for their needs.</p> </li> <li> <p>The script is streamed back to the user in real-time, allowing them to see the generated conversation between speakers</p> </li> </ul> <p>\ud83c\udf99\ufe0f Podcast Generation</p> <ul> <li> <p>For each speaker in the podcast script, audio is generated using the <code>text_to_speech</code> function with distinct speaker profiles</p> </li> <li> <p>The <code>SPEAKER_DESCRIPTION</code> enables the user to edit the podcast speakers voices to fit their needs.</p> </li> <li> <p>The generated audio is displayed with a player so users can listen directly in the app.</p> </li> </ul>"},{"location":"step-by-step-guide/#customizing-the-blueprint","title":"\ud83c\udfa8 Customizing the Blueprint","text":"<p>To better understand how you can tailor this Blueprint to suit your specific needs, please visit the Customization Guide.</p>"},{"location":"step-by-step-guide/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"}]}